{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# reading input log file\n",
    "# import ruamel.yaml\n",
    "import yaml\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import traceback\n",
    "from csv import excel\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "# test_name = \"1p9f9s_run\"\n",
    "# test_name = \"seq6f3s\"\n",
    "test_name = \"seq9f9s\"\n",
    "\n",
    "stat_path=f\"../example_stat/{test_name}\"\n",
    "iamge_path=f\"{stat_path}/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../example_stat/seq9f9s [] ['44025_vfd-data-stat-dl.yaml', 'seq9f9s-task_to_file.yaml', '44025_vol-data-stat-dl.yaml', 'RUN_TRACKING-dl.log']\n",
      "['../example_stat/seq9f9s/44025_vfd-data-stat-dl.yaml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def search_files_with_name(directory, pattern=\"\"):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        print(root, dirs, files)\n",
    "        for file in files:\n",
    "            if re.search(pattern, file):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "                #print(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "vfd_files = search_files_with_name(stat_path, \"vfd\")\n",
    "# vfd_files = vfd_files[0:1]\n",
    "print(vfd_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../example_stat/seq9f9s/44025_vfd-data-stat-dl.yaml\n",
      "dict_keys(['../example_stat/seq9f9s/44025_vfd-data-stat-dl.yaml'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_vfd_yaml(vfd_files):\n",
    "    # loag into {file_name:yaml_data} format\n",
    "    ret_dict = {}\n",
    "    tmp_dict = {}\n",
    "    for f in vfd_files:\n",
    "        with open(f, \"r\") as stream:\n",
    "            print(f\"loading {f}\")\n",
    "            try:\n",
    "                tmp_dict = yaml.safe_load(stream)\n",
    "                ret_dict[f] = tmp_dict\n",
    "                # print(tmp_dict)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "                print(\"Error loading yaml file\")\n",
    "                exit(1)\n",
    "    return ret_dict\n",
    "\n",
    "vfd_file_dict = load_vfd_yaml(vfd_files)\n",
    "print(vfd_file_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pid_from_file_path(file_path):\n",
    "    # Define the regular expression pattern to find numbers in the file path\n",
    "    pattern = r'\\d+'\n",
    "    # Use the re.findall function to find all occurrences of the pattern in the file path\n",
    "    numbers = re.findall(pattern, file_path)\n",
    "    # Since there might be multiple numbers in the file path, you can extract the last one\n",
    "    if numbers:\n",
    "        desired_number = int(numbers[-1])\n",
    "        print(\"task_pid:\", desired_number)\n",
    "    else:\n",
    "        print(\"No task_pid found in the file path.\")\n",
    "    return desired_number\n",
    "\n",
    "def get_op_blob_range(h5fd_stat):\n",
    "    # Format of io_stat\n",
    "    '''\n",
    "    read_bytes: 0\n",
    "        read_cnt: 0\n",
    "        read_ranges: {1:[0, 0]}\n",
    "        write_bytes: 7746756\n",
    "        write_cnt: 52\n",
    "        write_ranges:{2:[0, 0]}\n",
    "    '''\n",
    "    read_ranges = h5fd_stat['read_ranges']\n",
    "    write_ranges = h5fd_stat['write_ranges']\n",
    "    \n",
    "    if read_ranges:\n",
    "        start_op_idx = min(read_ranges.keys())\n",
    "        end_op_idx = max(read_ranges.keys())\n",
    "        start_blob = min(elem[0] for elem in list(read_ranges.values())) #min(read_ranges.values())\n",
    "        end_blob = max(elem[1] for elem in list(read_ranges.values()))\n",
    "    if write_ranges:\n",
    "        start_op_idx = min(write_ranges.keys())\n",
    "        end_op_idx = max(write_ranges.keys())\n",
    "        start_blob = min(elem[0] for elem in list(write_ranges.values())) #min(write_ranges.values())\n",
    "        end_blob = max(elem[1] for elem in list(write_ranges.values()))\n",
    "    \n",
    "    result = {\n",
    "        'start_op_idx': start_op_idx,\n",
    "        'end_op_idx': end_op_idx,\n",
    "        'start_blob': start_blob,\n",
    "        'end_blob': end_blob\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# TODO: need different get_min_max_op_idx\n",
    "def get_min_max_op_idx(io_stat):\n",
    "    # assumes we want the I/O operations and blobs for per file\n",
    "    min_op_idx = -1\n",
    "    max_op_idx = -1\n",
    "    min_blob = -1\n",
    "    max_blob = -1\n",
    "    for h5fd_type, h5fd_stat in io_stat.items():\n",
    "        res_stat = get_op_blob_range(h5fd_stat)\n",
    "        \n",
    "        if min_op_idx == -1:\n",
    "            min_op_idx = res_stat['start_op_idx']\n",
    "        elif res_stat['start_op_idx'] < min_op_idx:\n",
    "            min_op_idx = res_stat['start_op_idx']\n",
    "        \n",
    "        if max_op_idx == -1:\n",
    "            max_op_idx = res_stat['end_op_idx']\n",
    "        elif res_stat['end_op_idx'] > max_op_idx:\n",
    "            max_op_idx = res_stat['end_op_idx']\n",
    "        \n",
    "        if min_blob == -1:\n",
    "            min_blob = res_stat['start_blob']\n",
    "        elif res_stat['start_blob'] < min_blob:\n",
    "            min_blob = res_stat['start_blob']\n",
    "        \n",
    "        if max_blob == -1:\n",
    "            max_blob = res_stat['end_blob']\n",
    "        elif res_stat['end_blob'] > max_blob:\n",
    "            max_blob = res_stat['end_blob']\n",
    "    \n",
    "    result = {\n",
    "        'min_op_idx': min_op_idx,\n",
    "        'max_op_idx': max_op_idx,\n",
    "        'min_blob': min_blob,\n",
    "        'max_blob': max_blob\n",
    "    }\n",
    "    \n",
    "    # print(f\"h5fd_type: {h5fd_type}\")\n",
    "    # print(f\"res_stat: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Converting to Task-File dictionary\n",
    "def schema_to_task_file_dic(vfd_file_dict):\n",
    "    task_file_dict = {}\n",
    "    # Format of task_file_dict\n",
    "    '''\n",
    "    run_idfeature:\n",
    "        order: 0\n",
    "        task_pid: 75190\n",
    "        task_op_range: [0, 7208]\n",
    "        total_op_range: [0, 7208]\n",
    "        input:\n",
    "            - file_name: \"wrfout_rainrate_tb_zh_mh_2015-05-06_04:00:00.nc\"\n",
    "                file_op_range: [55, 71]\n",
    "        output:\n",
    "            - file_name: \"cloudid_20150506_040000.nc\"\n",
    "                ile_op_range: [0, 800]\n",
    "    '''\n",
    "    for file_path, file_data in vfd_file_dict.items():\n",
    "\n",
    "        task_pid = extract_pid_from_file_path(file_path)\n",
    "        \n",
    "        for li in file_data:\n",
    "            # print(li.keys())\n",
    "            if 'file' in list(li.keys())[0]:\n",
    "                key = list(li.keys())[0]\n",
    "                # print(li[key])\n",
    "                \n",
    "                if li[key]['file_type'] == 'na':\n",
    "                    print(\"Invalid Entry. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                task_name = li[key]['task_name']\n",
    "                file_name = li[key]['file_name']\n",
    "\n",
    "                if task_name not in task_file_dict.keys():\n",
    "                    # 1. Initialize task_file_dict\n",
    "                    task_file_dict[task_name] = {}\n",
    "                    task_file_dict[task_name]['order'] = 0 # placeholder\n",
    "                    task_file_dict[task_name]['task_pid'] = task_pid\n",
    "                    task_file_dict[task_name]['io_cnt'] = 0 # initial value\n",
    "                    # task_file_dict[task_name]['total_op_range'] = []\n",
    "                    task_file_dict[task_name]['input'] = {}\n",
    "                    task_file_dict[task_name]['output'] = {}\n",
    "                    \n",
    "                # 1. Update task op count\n",
    "                read_cnt = li[key]['file_read_cnt']\n",
    "                write_cnt = li[key]['file_write_cnt']\n",
    "                task_file_dict[task_name]['io_cnt'] += (read_cnt + write_cnt)\n",
    "                \n",
    "                # 2. Get blob and operation info\n",
    "                data_stat = li[key]['data']#['H5FD_MEM_DRAW']\n",
    "                meta_stats = li[key]['metadata']\n",
    "                data_stat.update(meta_stats)\n",
    "\n",
    "                # try: \n",
    "                # except:\n",
    "                #     print(\"Error:\")\n",
    "                #     print(f\"task_name: {task_name}\")\n",
    "                #     print(f\"file_name: {file_name}\")\n",
    "                io_stat = get_min_max_op_idx(data_stat)\n",
    "                \n",
    "                # 2. Add file to intput/output list\n",
    "                file_type = li[key]['file_type']\n",
    "                if file_type == 'input':\n",
    "                    # task_file_dict[task_name]['input'].append(file_name)\n",
    "                    task_file_dict[task_name]['input'][file_name] = io_stat\n",
    "                elif file_type == 'output':\n",
    "                    # task_file_dict[task_name]['output'].append(file_name)\n",
    "                    task_file_dict[task_name]['output'][file_name] = io_stat\n",
    "                # TODO: what if read_write file occur?\n",
    "                \n",
    "\n",
    "                # print(f\"{file_name}: {io_stat}\")\n",
    "                # data_stat_range_list = list(data_stat['write_ranges'].values())\n",
    "                \n",
    "    return task_file_dict\n",
    "\n",
    "def sort_task_in_order(task_file_dict, task_order_list):\n",
    "    op_offset = 0\n",
    "    for i, task_name in enumerate(task_order_list):\n",
    "        # if task_name in task_file_dict.keys():\n",
    "        task_file_dict[task_name]['order'] = i\n",
    "        io_cnt = task_file_dict[task_name]['io_cnt']\n",
    "        # task_file_dict[task_name]['total_op_range'] = f\"[{op_offset},{io_cnt}]\"\n",
    "        task_file_dict[task_name]['total_op_range'] = {}\n",
    "        \n",
    "        task_file_dict[task_name]['total_op_range']['start'] = op_offset\n",
    "        task_file_dict[task_name]['total_op_range']['end'] = op_offset + io_cnt\n",
    "\n",
    "        op_offset += io_cnt\n",
    "    return task_file_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_pid: 44025\n",
      "Invalid Entry. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# Save the task to input/output file mapping\n",
    "def save_task_file_dict(task_file_dict, test_name):\n",
    "    tf_file_path = stat_path=f\"../example_stat/{test_name}/{test_name}-task_to_file.yaml\"\n",
    "    \n",
    "    # if os.path.exists(tf_file_path):\n",
    "    #     # Remove the file\n",
    "    #     os.remove(tf_file_path)\n",
    "    \n",
    "    with open(tf_file_path, 'w') as yaml_file:\n",
    "        for task_name,data in task_file_dict.items():\n",
    "            input_files = list(data['input'].keys())\n",
    "            output_files = list(data['output'].keys())\n",
    "            item_dict = {\n",
    "                task_name: {\n",
    "                    'order': data['order'],\n",
    "                    'task_pid': data['task_pid'], # TODO: not need?\n",
    "                    'io_cnt': data['io_cnt'], # TODO: not need?\n",
    "                    'input': input_files,\n",
    "                    'output': output_files\n",
    "                }\n",
    "            }\n",
    "            yaml.dump(item_dict, yaml_file)\n",
    "\n",
    "# task_order_list = [ \"run_idfeature\", \"run_tracksingle\", \"run_gettracks\", \"run_trackstats\", \"run_identifymcs\", \"run_matchpf\", \"run_mcsstats\", \"run_robustmcs\", \"run_mapfeature\", \"run_speed\"]\n",
    "\n",
    "task_file_dict = schema_to_task_file_dic(vfd_file_dict)\n",
    "task_order_list = [ \"run_idfeature\", \"run_tracksingle\", \"run_gettracks\"]\n",
    "ordered_task_file_dict = sort_task_in_order(task_file_dict, task_order_list)\n",
    "save_task_file_dict(task_file_dict, test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to prefetcher format\n",
    "\n",
    "def save_hermes_prefetch(task_file_dict, test_name):\n",
    "\n",
    "    prefetch_file_path = f\"../example_stat/{test_name}/apriori_{test_name}.yaml\"\n",
    "\n",
    "    with open(prefetch_file_path, 'w') as file:\n",
    "        file.write(\"0:\\n\")\n",
    "        # for task_name,data in task_file_dict.items():\n",
    "        for data in task_file_dict.values():\n",
    "            for input_file,file_stat in data['input'].items():\n",
    "                op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "                blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "                file.write(f\"  - bucket: {input_file}\\n\")\n",
    "                file.write(f\"    prefetch:\\n\")\n",
    "                file.write(f\"    - op_count_range: {op_range}\\n\")\n",
    "                file.write(f\"      promote_blobs: {blob_range}\\n\")\n",
    "            for output_file,file_stat in data['input'].items():\n",
    "                op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "                blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "                file.write(f\"  - bucket: {output_file}\\n\")\n",
    "                file.write(f\"    prefetch:\\n\")\n",
    "                file.write(f\"    - op_count_range: {op_range}\\n\")\n",
    "                file.write(f\"      demote_blobs: {blob_range}\\n\")\n",
    "            # print(input_file)\n",
    "            # print(file_stat)\n",
    "\n",
    "save_hermes_prefetch(task_file_dict, test_name)    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
