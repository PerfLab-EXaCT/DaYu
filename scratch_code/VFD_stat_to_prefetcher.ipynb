{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# reading input log file\n",
    "# import ruamel.yaml\n",
    "import yaml\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import traceback\n",
    "from csv import excel\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "# test_name = \"1p9f9s_run\"\n",
    "test_name = \"seq6f3s\"\n",
    "\n",
    "stat_path=f\"../example_stat/{test_name}\"\n",
    "iamge_path=f\"{stat_path}/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../example_stat/seq6f3s [] ['apriori_seq6f3s.yaml', '32510_vfd-data-stat-dl.yaml', 'seq6f3s-task_to_file.yaml', '32510_vol-data-stat-dl.yaml']\n",
      "['../example_stat/seq6f3s/32510_vfd-data-stat-dl.yaml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def search_files_with_name(directory, pattern=\"\"):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        print(root, dirs, files)\n",
    "        for file in files:\n",
    "            if re.search(pattern, file):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "                #print(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "vfd_files = search_files_with_name(stat_path, \"vfd\")\n",
    "# vfd_files = vfd_files[0:1]\n",
    "print(vfd_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../example_stat/seq6f3s/32510_vfd-data-stat-dl.yaml\n",
      "dict_keys(['../example_stat/seq6f3s/32510_vfd-data-stat-dl.yaml'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_vfd_yaml(vfd_files):\n",
    "    # loag into {file_name:yaml_data} format\n",
    "    ret_dict = {}\n",
    "    tmp_dict = {}\n",
    "    for f in vfd_files:\n",
    "        with open(f, \"r\") as stream:\n",
    "            print(f\"loading {f}\")\n",
    "            try:\n",
    "                tmp_dict = yaml.safe_load(stream)\n",
    "                ret_dict[f] = tmp_dict\n",
    "                # print(tmp_dict)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "                print(\"Error loading yaml file\")\n",
    "                exit(1)\n",
    "    return ret_dict\n",
    "\n",
    "vfd_file_dict = load_vfd_yaml(vfd_files)\n",
    "print(vfd_file_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_order_list = [ \"run_idfeature\", \"run_tracksingle\", \"run_gettracks\", \"run_trackstats\", \"run_identifymcs\", \"run_matchpf\", \"run_mcsstats\", \"run_robustmcs\", \"run_mapfeature\", \"run_speed\"]\n",
    "\n",
    "\n",
    "def extract_pid_from_file_path(file_path):\n",
    "    # Define the regular expression pattern to find numbers in the file path\n",
    "    pattern = r'\\d+'\n",
    "    # Use the re.findall function to find all occurrences of the pattern in the file path\n",
    "    numbers = re.findall(pattern, file_path)\n",
    "    # Since there might be multiple numbers in the file path, you can extract the last one\n",
    "    if numbers:\n",
    "        desired_number = int(numbers[-1])\n",
    "        print(\"task_pid:\", desired_number)\n",
    "    else:\n",
    "        print(\"No task_pid found in the file path.\")\n",
    "    return desired_number\n",
    "\n",
    "def get_op_blob_range(h5fd_stat):\n",
    "    # Format of io_stat\n",
    "    '''\n",
    "    read_bytes: 0\n",
    "        read_cnt: 0\n",
    "        read_ranges: {1:[0, 0]}\n",
    "        write_bytes: 7746756\n",
    "        write_cnt: 52\n",
    "        write_ranges:{2:[0, 0]}\n",
    "    '''\n",
    "    read_ranges = h5fd_stat['read_ranges']\n",
    "    write_ranges = h5fd_stat['write_ranges']\n",
    "    \n",
    "    if read_ranges:\n",
    "        start_op_idx = min(read_ranges.keys())\n",
    "        end_op_idx = max(read_ranges.keys())\n",
    "        start_blob = min(elem[0] for elem in list(read_ranges.values())) #min(read_ranges.values())\n",
    "        end_blob = max(elem[1] for elem in list(read_ranges.values()))\n",
    "    if write_ranges:\n",
    "        start_op_idx = min(write_ranges.keys())\n",
    "        end_op_idx = max(write_ranges.keys())\n",
    "        start_blob = min(elem[0] for elem in list(write_ranges.values())) #min(write_ranges.values())\n",
    "        end_blob = max(elem[1] for elem in list(write_ranges.values()))\n",
    "    \n",
    "    result = {\n",
    "        'start_op_idx': start_op_idx,\n",
    "        'end_op_idx': end_op_idx,\n",
    "        'start_blob': start_blob,\n",
    "        'end_blob': end_blob\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# TODO: need different get_min_max_op_idx\n",
    "def get_min_max_op_idx(io_stat):\n",
    "    # assumes we want the I/O operations and blobs for per file\n",
    "    min_op_idx = -1\n",
    "    max_op_idx = -1\n",
    "    min_blob = -1\n",
    "    max_blob = -1\n",
    "    for h5fd_type, h5fd_stat in io_stat.items():\n",
    "        res_stat = get_op_blob_range(h5fd_stat)\n",
    "        \n",
    "        if min_op_idx == -1:\n",
    "            min_op_idx = res_stat['start_op_idx']\n",
    "        elif res_stat['start_op_idx'] < min_op_idx:\n",
    "            min_op_idx = res_stat['start_op_idx']\n",
    "        \n",
    "        if max_op_idx == -1:\n",
    "            max_op_idx = res_stat['end_op_idx']\n",
    "        elif res_stat['end_op_idx'] > max_op_idx:\n",
    "            max_op_idx = res_stat['end_op_idx']\n",
    "        \n",
    "        if min_blob == -1:\n",
    "            min_blob = res_stat['start_blob']\n",
    "        elif res_stat['start_blob'] < min_blob:\n",
    "            min_blob = res_stat['start_blob']\n",
    "        \n",
    "        if max_blob == -1:\n",
    "            max_blob = res_stat['end_blob']\n",
    "        elif res_stat['end_blob'] > max_blob:\n",
    "            max_blob = res_stat['end_blob']\n",
    "    \n",
    "    result = {\n",
    "        'min_op_idx': min_op_idx,\n",
    "        'max_op_idx': max_op_idx,\n",
    "        'min_blob': min_blob,\n",
    "        'max_blob': max_blob\n",
    "    }\n",
    "    \n",
    "    # print(f\"h5fd_type: {h5fd_type}\")\n",
    "    # print(f\"res_stat: {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Converting to Task-File dictionary\n",
    "def schema_tp_task_file_dic(vfd_file_dict):\n",
    "    task_file_dict = {}\n",
    "    # Format of task_file_dict\n",
    "    '''\n",
    "    run_idfeature:\n",
    "        order: 0\n",
    "        task_pid: 75190\n",
    "        task_op_range: [0, 7208]\n",
    "        total_op_range: [0, 7208]\n",
    "        input:\n",
    "            - file_name: \"wrfout_rainrate_tb_zh_mh_2015-05-06_04:00:00.nc\"\n",
    "                file_op_range: [55, 71]\n",
    "        output:\n",
    "            - file_name: \"cloudid_20150506_040000.nc\"\n",
    "                ile_op_range: [0, 800]\n",
    "    '''\n",
    "    for file_path, file_data in vfd_file_dict.items():\n",
    "\n",
    "        task_pid = extract_pid_from_file_path(file_path)\n",
    "        \n",
    "        for li in file_data:\n",
    "            # print(li.keys())\n",
    "            if 'file' in list(li.keys())[0]:\n",
    "                key = list(li.keys())[0]\n",
    "                # print(li[key])\n",
    "                task_name = li[key]['task_name']\n",
    "                file_name = li[key]['file_name']\n",
    "\n",
    "                if task_name not in task_file_dict.keys():\n",
    "                    # 1. Initialize task_file_dict\n",
    "                    task_file_dict[task_name] = {}\n",
    "                    task_file_dict[task_name]['order'] = 0 # placeholder\n",
    "                    task_file_dict[task_name]['task_pid'] = task_pid\n",
    "                    task_file_dict[task_name]['io_cnt'] = 0 # initial value\n",
    "                    # task_file_dict[task_name]['total_op_range'] = []\n",
    "                    task_file_dict[task_name]['input'] = {}\n",
    "                    task_file_dict[task_name]['output'] = {}\n",
    "                    \n",
    "                # 1. Update task op count\n",
    "                read_cnt = li[key]['file_read_cnt']\n",
    "                write_cnt = li[key]['file_write_cnt']\n",
    "                task_file_dict[task_name]['io_cnt'] += (read_cnt + write_cnt)\n",
    "                \n",
    "                # 2. Get blob and operation info\n",
    "                data_stat = li[key]['data']#['H5FD_MEM_DRAW']\n",
    "                meta_stats = li[key]['metadata']\n",
    "                data_stat.update(meta_stats)\n",
    "                io_stat = get_min_max_op_idx(data_stat)\n",
    "                \n",
    "                # 2. Add file to intput/output list\n",
    "                file_type = li[key]['file_type']\n",
    "                if file_type == 'input':\n",
    "                    # task_file_dict[task_name]['input'].append(file_name)\n",
    "                    task_file_dict[task_name]['input'][file_name] = io_stat\n",
    "                elif file_type == 'output':\n",
    "                    # task_file_dict[task_name]['output'].append(file_name)\n",
    "                    task_file_dict[task_name]['output'][file_name] = io_stat\n",
    "                # TODO: what if read_write file occur?\n",
    "                \n",
    "\n",
    "                # print(f\"{file_name}: {io_stat}\")\n",
    "                # data_stat_range_list = list(data_stat['write_ranges'].values())\n",
    "                \n",
    "    return task_file_dict\n",
    "\n",
    "def sort_task_in_order(task_file_dict, task_order_list):\n",
    "    op_offset = 0\n",
    "    for i, task_name in enumerate(task_order_list):\n",
    "        # if task_name in task_file_dict.keys():\n",
    "        task_file_dict[task_name]['order'] = i\n",
    "        io_cnt = task_file_dict[task_name]['io_cnt']\n",
    "        # task_file_dict[task_name]['total_op_range'] = f\"[{op_offset},{io_cnt}]\"\n",
    "        task_file_dict[task_name]['total_op_range'] = {}\n",
    "        \n",
    "        task_file_dict[task_name]['total_op_range']['start'] = op_offset\n",
    "        task_file_dict[task_name]['total_op_range']['end'] = op_offset + io_cnt\n",
    "\n",
    "        op_offset += io_cnt\n",
    "    return task_file_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_pid: 32510\n",
      "run_idfeature:\n",
      "    {'order': 0, 'task_pid': 32510, 'io_cnt': 4824, 'input': {'wrfout_rainrate_tb_zh_mh_2015-05-06_00:00:00.nc': {'min_op_idx': 0, 'max_op_idx': 74, 'min_blob': 0, 'max_blob': 33}, 'wrfout_rainrate_tb_zh_mh_2015-05-06_01:00:00.nc': {'min_op_idx': 804, 'max_op_idx': 878, 'min_blob': 0, 'max_blob': 33}, 'wrfout_rainrate_tb_zh_mh_2015-05-06_02:00:00.nc': {'min_op_idx': 1608, 'max_op_idx': 1682, 'min_blob': 0, 'max_blob': 32}, 'wrfout_rainrate_tb_zh_mh_2015-05-06_03:00:00.nc': {'min_op_idx': 2412, 'max_op_idx': 2486, 'min_blob': 0, 'max_blob': 31}, 'wrfout_rainrate_tb_zh_mh_2015-05-06_04:00:00.nc': {'min_op_idx': 3216, 'max_op_idx': 3290, 'min_blob': 0, 'max_blob': 31}, 'wrfout_rainrate_tb_zh_mh_2015-05-06_05:00:00.nc': {'min_op_idx': 4020, 'max_op_idx': 4094, 'min_blob': 0, 'max_blob': 30}}, 'output': {'cloudid_20150506_000000.nc': {'min_op_idx': 75, 'max_op_idx': 803, 'min_blob': 0, 'max_blob': 14}, 'cloudid_20150506_010000.nc': {'min_op_idx': 879, 'max_op_idx': 1607, 'min_blob': 0, 'max_blob': 14}, 'cloudid_20150506_020000.nc': {'min_op_idx': 1683, 'max_op_idx': 2411, 'min_blob': 0, 'max_blob': 14}, 'cloudid_20150506_030000.nc': {'min_op_idx': 2487, 'max_op_idx': 3215, 'min_blob': 0, 'max_blob': 13}, 'cloudid_20150506_040000.nc': {'min_op_idx': 3291, 'max_op_idx': 4019, 'min_blob': 0, 'max_blob': 13}, 'cloudid_20150506_050000.nc': {'min_op_idx': 4095, 'max_op_idx': 4823, 'min_blob': 0, 'max_blob': 13}}, 'total_op_range': {'start': 0, 'end': 4824}}\n",
      "run_tracksingle:\n",
      "    {'order': 1, 'task_pid': 32510, 'io_cnt': 3535, 'input': {'cloudid_20150506_000000.nc': {'min_op_idx': 5206, 'max_op_idx': 5329, 'min_blob': 0, 'max_blob': 14}, 'cloudid_20150506_010000.nc': {'min_op_idx': 5913, 'max_op_idx': 6036, 'min_blob': 0, 'max_blob': 14}, 'cloudid_20150506_020000.nc': {'min_op_idx': 6620, 'max_op_idx': 6743, 'min_blob': 0, 'max_blob': 14}, 'cloudid_20150506_030000.nc': {'min_op_idx': 7327, 'max_op_idx': 7450, 'min_blob': 0, 'max_blob': 13}, 'cloudid_20150506_040000.nc': {'min_op_idx': 8034, 'max_op_idx': 8157, 'min_blob': 0, 'max_blob': 13}, 'cloudid_20150506_050000.nc': {'min_op_idx': 7910, 'max_op_idx': 8033, 'min_blob': 0, 'max_blob': 13}}, 'output': {'track_20150506_010000.nc': {'min_op_idx': 5330, 'max_op_idx': 5530, 'min_blob': 0, 'max_blob': 0}, 'track_20150506_020000.nc': {'min_op_idx': 6037, 'max_op_idx': 6237, 'min_blob': 0, 'max_blob': 0}, 'track_20150506_030000.nc': {'min_op_idx': 6744, 'max_op_idx': 6944, 'min_blob': 0, 'max_blob': 0}, 'track_20150506_040000.nc': {'min_op_idx': 7451, 'max_op_idx': 7651, 'min_blob': 0, 'max_blob': 0}, 'track_20150506_050000.nc': {'min_op_idx': 8158, 'max_op_idx': 8358, 'min_blob': 0, 'max_blob': 0}}, 'total_op_range': {'start': 4824, 'end': 8359}}\n",
      "run_gettracks:\n",
      "    {'order': 2, 'task_pid': 32510, 'io_cnt': 2050, 'input': {'track_20150506_010000.nc': {'min_op_idx': 8413, 'max_op_idx': 8532, 'min_blob': 0, 'max_blob': 0}, 'cloudid_20150506_000000.nc': {'min_op_idx': 8533, 'max_op_idx': 8656, 'min_blob': 0, 'max_blob': 14}, 'cloudid_20150506_010000.nc': {'min_op_idx': 8901, 'max_op_idx': 9024, 'min_blob': 0, 'max_blob': 14}, 'track_20150506_020000.nc': {'min_op_idx': 8781, 'max_op_idx': 8900, 'min_blob': 0, 'max_blob': 0}, 'cloudid_20150506_020000.nc': {'min_op_idx': 9269, 'max_op_idx': 9392, 'min_blob': 0, 'max_blob': 14}, 'track_20150506_030000.nc': {'min_op_idx': 9149, 'max_op_idx': 9268, 'min_blob': 0, 'max_blob': 0}, 'cloudid_20150506_030000.nc': {'min_op_idx': 9637, 'max_op_idx': 9760, 'min_blob': 0, 'max_blob': 13}, 'track_20150506_040000.nc': {'min_op_idx': 9517, 'max_op_idx': 9636, 'min_blob': 0, 'max_blob': 0}, 'cloudid_20150506_040000.nc': {'min_op_idx': 10005, 'max_op_idx': 10128, 'min_blob': 0, 'max_blob': 13}, 'track_20150506_050000.nc': {'min_op_idx': 9885, 'max_op_idx': 10004, 'min_blob': 0, 'max_blob': 0}, 'cloudid_20150506_050000.nc': {'min_op_idx': 10129, 'max_op_idx': 10252, 'min_blob': 0, 'max_blob': 13}}, 'output': {'tracknumbers_20150506.0000_20150506.0500.nc': {'min_op_idx': 10253, 'max_op_idx': 10408, 'min_blob': 0, 'max_blob': 0}}, 'total_op_range': {'start': 8359, 'end': 10409}}\n"
     ]
    }
   ],
   "source": [
    "task_file_dict = schema_tp_task_file_dic(vfd_file_dict)\n",
    "task_order_list = [ \"run_idfeature\", \"run_tracksingle\", \"run_gettracks\"]\n",
    "ordered_task_file_dict = sort_task_in_order(task_file_dict, task_order_list)\n",
    "\n",
    "for task_name,data in task_file_dict.items():\n",
    "    print(f\"{task_name}:\")\n",
    "    print(f\"    {data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the task to input/output file mapping\n",
    "def save_task_file_dict(task_file_dict, test_name):\n",
    "    tf_file_path = stat_path=f\"../example_stat/{test_name}/{test_name}-task_to_file.yaml\"\n",
    "    \n",
    "    # if os.path.exists(tf_file_path):\n",
    "    #     # Remove the file\n",
    "    #     os.remove(tf_file_path)\n",
    "    \n",
    "    with open(tf_file_path, 'w') as yaml_file:\n",
    "        for task_name,data in task_file_dict.items():\n",
    "            input_files = list(data['input'].keys())\n",
    "            output_files = list(data['output'].keys())\n",
    "            item_dict = {\n",
    "                task_name: {\n",
    "                    'order': data['order'],\n",
    "                    'task_pid': data['task_pid'], # TODO: not need?\n",
    "                    'io_cnt': data['io_cnt'], # TODO: not need?\n",
    "                    'input': input_files,\n",
    "                    'output': output_files\n",
    "                }\n",
    "            }\n",
    "            yaml.dump(item_dict, yaml_file)\n",
    "\n",
    "save_task_file_dict(task_file_dict, test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to prefetcher format\n",
    "# TODO: Need to convert to file_name based dictionary first\n",
    "\n",
    "prefetch_file_path = stat_path=f\"../example_stat/{test_name}/apriori_{test_name}.yaml\"\n",
    "file_op_dict = {}\n",
    "\n",
    "# if os.path.exists(prefetch_file_path):\n",
    "#     # Remove the file\n",
    "#     os.remove(prefetch_file_path)\n",
    "\n",
    "with open(prefetch_file_path, 'w') as file:\n",
    "    file.write(\"0:\\n\")\n",
    "    for task_name,data in task_file_dict.items():\n",
    "        for input_file,file_stat in data['input'].items():\n",
    "            op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "            blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "            file.write(f\"  - bucket: {input_file}\\n\")\n",
    "            file.write(f\"    prefetch:\\n\")\n",
    "            file.write(f\"    - op_count_range: {op_range}\\n\")\n",
    "            file.write(f\"      promote_blobs: {blob_range}\\n\")\n",
    "        for output_file,file_stat in data['input'].items():\n",
    "            op_range = f\"[{file_stat['min_op_idx']},{file_stat['max_op_idx']}]\"\n",
    "            blob_range = f\"[{file_stat['min_blob']},{file_stat['max_blob']}]\"\n",
    "            file.write(f\"  - bucket: {input_file}\\n\")\n",
    "            file.write(f\"    prefetch:\\n\")\n",
    "            file.write(f\"    - op_count_range: {op_range}\\n\")\n",
    "            file.write(f\"      demote_blobs: {blob_range}\\n\")\n",
    "            # print(input_file)\n",
    "            # print(file_stat)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op_range_shift_dict = {\n",
    "#     \"75190\":0,\n",
    "#     \"5904\":7208,\n",
    "#     \"77100\":12689,\n",
    "#     \"168881\":15641,\n",
    "#     \"98522\":30542,\n",
    "#     \"170958\":31599,\n",
    "#     \"11179\":34399,\n",
    "#     \"157121\":37801,\n",
    "#     # \"167838\":91681,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# task_pid_dict = {\n",
    "#     \"run_idfeature\": {\"task_pid\":75190},\n",
    "#     \"run_tracksingle\": {\"task_pid\":5904},\n",
    "#     \"run_gettracks\": {\"task_pid\":77100},\n",
    "#     \"run_trackstats\": {\"task_pid\":168881},\n",
    "#     \"run_identifymcs\": {\"task_pid\":98522},\n",
    "#     \"run_matchpf\": {\"task_pid\":170958},\n",
    "#     \"run_mcsstats\": {\"task_pid\":11179},\n",
    "#     \"run_robustmcs\": {\"task_pid\":157121},\n",
    "#     # \"run_mapfeature\": {\"task_pid\":167838},\n",
    "#     # \"run_speed\": {\"task_pid\":??},\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# prefetch_entry_list = []\n",
    "\n",
    "# # get the I/O range\n",
    "# for k,stat in vfd_dict.items():\n",
    "#     # print(f\"VFD Stat File: {k}\")\n",
    "#     task_max_io = 0\n",
    "#     prev_file_max_io = 0\n",
    "#     file_op_range_dict = {}\n",
    "#     for li in stat:\n",
    "#         file_key = list(li.keys())[0]\n",
    "#         if 'file' in file_key:\n",
    "#             # file_stat = list(li.values())[0]\n",
    "#             file_stat = li[file_key]\n",
    "#             file_name = file_stat['file_name']\n",
    "#             file_max_io = 0\n",
    "#             file_min_io = prev_file_max_io\n",
    "#             file_max_blob = 0\n",
    "#             prefetch_type = 0 # 0: no prefetch, 1: prefetch blobs\n",
    "#             try:\n",
    "#                 h5mDraw = file_stat['data']['H5FD_MEM_DRAW']\n",
    "                \n",
    "#                 if h5mDraw['read_cnt'] > 0:\n",
    "#                     # print(f\"blobs here: {[val[1] for val in h5mDraw['read_ranges'].values()]}\")\n",
    "#                     max_blob = max([val[1] for val in h5mDraw['read_ranges'].values()])\n",
    "#                     max_io = max(h5mDraw['read_ranges'].keys())\n",
    "#                     min_io = min(h5mDraw['read_ranges'].keys())\n",
    "#                     if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "#                     if max_io > task_max_io: task_max_io = max_io\n",
    "#                     if max_io > file_max_io: file_max_io = max_io\n",
    "#                     if min_io < file_min_io: file_min_io = min_io\n",
    "#                     prefetch_type = 1\n",
    "                    \n",
    "#                 elif h5mDraw['write_cnt'] > 0:\n",
    "#                     # print(f\"blobs here: {[val[1] for val in h5mDraw['write_ranges'].values()]}\")\n",
    "#                     max_blob = max([val[1] for val in h5mDraw['write_ranges'].values()])\n",
    "#                     max_io = max(h5mDraw['write_ranges'].keys())\n",
    "#                     min_io = min(h5mDraw['read_ranges'].keys())\n",
    "#                     if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "#                     if max_io > task_max_io: task_max_io = max_io\n",
    "#                     if max_io > file_max_io: file_max_io = max_io\n",
    "#                     if min_io < file_min_io: file_min_io = min_io\n",
    "#                     prefetch_type = 0\n",
    "#                 else:\n",
    "#                     pass\n",
    "                \n",
    "#                 for meta_type, meta_stats in file_stat['metadata'].items():\n",
    "#                     # print(f\"meta_type: {meta_type}\")\n",
    "#                     if file_stat['metadata'][meta_type]['read_cnt'] > 0:\n",
    "#                         max_blob = max([val[1] for val in file_stat['metadata'][meta_type]['read_ranges'].values()])\n",
    "#                         max_io = max(file_stat['metadata'][meta_type]['read_ranges'].keys())\n",
    "#                         min_io = min(file_stat['metadata'][meta_type]['read_ranges'].keys())\n",
    "#                         if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "#                         if max_io > task_max_io: task_max_io = max_io\n",
    "#                         if max_io > file_max_io: file_max_io = max_io\n",
    "#                         if min_io < file_min_io: file_min_io = min_io\n",
    "#                         prefetch_type = 1\n",
    "                        \n",
    "#                     elif file_stat['metadata'][meta_type]['write_cnt'] > 0:\n",
    "#                         max_blob = max([val[1] for val in file_stat['metadata'][meta_type]['write_ranges'].values()])\n",
    "#                         max_io = max(file_stat['metadata'][meta_type]['write_ranges'].keys())\n",
    "#                         min_io = min(file_stat['metadata'][meta_type]['write_ranges'].keys())\n",
    "#                         if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "#                         if max_io > task_max_io: task_max_io = max_io\n",
    "#                         if max_io > file_max_io: file_max_io = max_io\n",
    "#                         if min_io < file_min_io: file_min_io = min_io\n",
    "#                         prefetch_type = 0\n",
    "#                     else:\n",
    "#                         pass\n",
    "                \n",
    "#                 # if \"cloudid_20150506_040000\" in file_name:\n",
    "#                 #     print(f\"VFD Stat File: {k}\")\n",
    "#                 #     print(f\"file_name: {file_name}\")\n",
    "#                 #     print(f\"file_io_range: [{file_min_io},{file_max_io}]\")\n",
    "#                 if file_name in file_op_range_dict.keys():\n",
    "#                     file_op_range_dict[file_name]['io_ranges'] .append((file_min_io, file_max_io))\n",
    "#                     if file_max_blob != file_op_range_dict[file_name]['blob_reanges'][1]:\n",
    "#                         file_op_range_dict[file_name]['blob_reanges'] = (0, file_max_blob)\n",
    "#                         file_op_range_dict[file_name]['prefetch_type'] = prefetch_type\n",
    "                        \n",
    "#                 else:\n",
    "#                     file_op_range_dict[file_name] = {}\n",
    "#                     file_op_range_dict[file_name]['io_ranges'] = [(file_min_io, file_max_io)]\n",
    "#                     file_op_range_dict[file_name]['blob_reanges'] = (0, file_max_blob)\n",
    "#                     file_op_range_dict[file_name]['prefetch_type'] = prefetch_type\n",
    "            \n",
    "#                 prev_file_max_io = file_max_io\n",
    "#                 if prefetch_type == 0:\n",
    "#                     print(f\"file_op_range_dict: {file_op_range_dict[file_name]}\")\n",
    "\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "\n",
    "#     file_name = k.split('/')[-1]\n",
    "#     task_id = file_name.split('_')[0]\n",
    "#     # print(f\"{task_id} Max IO IDX: {task_max_io}\")\n",
    "#     range_shift = op_range_shift_dict[task_id]\n",
    "#     page_size_factor = 1 # comparing to 1M, 512KB is 2 for 1M\n",
    "    \n",
    "#     for file_name, ranges_val in file_op_range_dict.items():\n",
    "#         for range in ranges_val['io_ranges']:\n",
    "#             pre_entry = {}\n",
    "#             # pre_entry['op_count_range'] = f\"[{page_size_factor*(range_shift+0)}, {page_size_factor*(range_shift+task_max_io)}]\"\n",
    "#             # pre_entry['op_count_range'] = [page_size_factor*(range_shift+range[0]), page_size_factor*(range_shift+range[1])]\n",
    "#             pre_entry['op_count_range'] = [page_size_factor*(range_shift), page_size_factor*(range_shift)]\n",
    "\n",
    "\n",
    "#             pre_entry['prefetch'] = []\n",
    "#             bucket_entry = {}\n",
    "#             bucket_entry['bucket'] = file_name\n",
    "#             # bucket_entry['promote_blobs'] = f\"[{page_size_factor*(ranges_val['blob_reanges'][0])}, {page_size_factor*(ranges_val['blob_reanges'][1])}]\"\n",
    "#             bucket_entry['blob_ranges'] = [page_size_factor*(ranges_val['blob_reanges'][0]), page_size_factor*(ranges_val['blob_reanges'][1])]\n",
    "#             bucket_entry['prefetch_type'] = \"promote_blobs\" if ranges_val['prefetch_type'] == 1 else \"demote_blobs\"\n",
    "            \n",
    "#             pre_entry['prefetch'].append(bucket_entry)\n",
    "#             prefetch_entry_list.append(pre_entry)\n",
    "            \n",
    "\n",
    "# with open('./apriori_p-1p9f-512KB.yml', 'w') as outfile:\n",
    "#     outfile.writelines(\"0:\\n\")\n",
    "#     sorted_entries = sorted(prefetch_entry_list, key=lambda x: x['op_count_range'][0])\n",
    "\n",
    "#     for ent in sorted_entries:\n",
    "#         # print(f\"  - op_count_range: {ent['op_count_range']}\")\n",
    "#         # print(f\"    prefetch:\")\n",
    "        \n",
    "#         outfile.writelines(f\"  - op_count_range: {ent['op_count_range']}\\n\")\n",
    "#         outfile.writelines(f\"    prefetch:\\n\")\n",
    "#         for bucket in ent['prefetch']:\n",
    "#             # print(f\"      - bucket: \\\"{bucket['bucket']}\\\"\")\n",
    "#             # print(f\"        promote_blobs: {bucket['promote_blobs']}\")\n",
    "#             # TODO: bucketname correction\n",
    "#             prefix = bucket['bucket'].split('_')[0]\n",
    "#             if prefix == \"cloudid\" or prefix == \"track\":\n",
    "#                 prefix_path = \"/qfs/projects/oddite/tang584/flextrkr_runs/hm_wrf_tbradar/tracking\"\n",
    "#             if prefix == \"mcs\" or prefix == \"trackstats\":\n",
    "#                 prefix_path = \"/qfs/projects/oddite/tang584/flextrkr_runs/hm_wrf_tbradar/stats\"\n",
    "#             if prefix == \"wrfout\" or prefix == \"wrf\":\n",
    "#                 prefix_path = \"/qfs/projects/oddite/tang584/flextrkr_runs/hm_input_data/wrf_tbradar\"\n",
    "#             bucket_name = prefix_path + \"/\" + bucket['bucket']\n",
    "#             outfile.writelines(f\"      - bucket: \\\"{bucket_name}\\\"\\n\")\n",
    "#             outfile.writelines(f\"        {bucket['prefetch_type']}: {bucket['blob_ranges']}\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'32510'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/mengtang/Desktop/projects/pnnl/vol-tracker/stat_to_prefetch/VFD_stat_to_prefetcher.ipynb Cell 9\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mengtang/Desktop/projects/pnnl/vol-tracker/stat_to_prefetch/VFD_stat_to_prefetcher.ipynb#W4sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m task_id \u001b[39m=\u001b[39m file_name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mengtang/Desktop/projects/pnnl/vol-tracker/stat_to_prefetch/VFD_stat_to_prefetcher.ipynb#W4sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39m# print(f\"{task_id} Max IO IDX: {task_max_io}\")\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mengtang/Desktop/projects/pnnl/vol-tracker/stat_to_prefetch/VFD_stat_to_prefetcher.ipynb#W4sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m range_shift \u001b[39m=\u001b[39m op_range_shift_dict[task_id]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mengtang/Desktop/projects/pnnl/vol-tracker/stat_to_prefetch/VFD_stat_to_prefetcher.ipynb#W4sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m page_size_factor \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# comparing to 1M, 512KB is 2 for 1M\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mengtang/Desktop/projects/pnnl/vol-tracker/stat_to_prefetch/VFD_stat_to_prefetcher.ipynb#W4sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_name, ranges_val \u001b[39min\u001b[39;00m file_op_range_dict\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyError\u001b[0m: '32510'"
     ]
    }
   ],
   "source": [
    "op_range_shift_dict = {\n",
    "    \"75190\":0,\n",
    "    \"5904\":7208,\n",
    "    \"77100\":12689,\n",
    "    \"168881\":15641,\n",
    "    \"98522\":30542,\n",
    "    \"170958\":31599,\n",
    "    \"11179\":34399,\n",
    "    \"157121\":37801,\n",
    "    # \"167838\":91681,\n",
    "}\n",
    "\n",
    "task_pid_dict = {\n",
    "    \"run_idfeature\": {\"task_pid\":75190},\n",
    "    \"run_tracksingle\": {\"task_pid\":5904},\n",
    "    \"run_gettracks\": {\"task_pid\":77100},\n",
    "    \"run_trackstats\": {\"task_pid\":168881},\n",
    "    \"run_identifymcs\": {\"task_pid\":98522},\n",
    "    \"run_matchpf\": {\"task_pid\":170958},\n",
    "    \"run_mcsstats\": {\"task_pid\":11179},\n",
    "    \"run_robustmcs\": {\"task_pid\":157121},\n",
    "    # \"run_mapfeature\": {\"task_pid\":167838},\n",
    "    # \"run_speed\": {\"task_pid\":??},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "prefetch_entry_list = []\n",
    "\n",
    "# get the I/O range\n",
    "for k,stat in vfd_dict.items():\n",
    "    # print(f\"VFD Stat File: {k}\")\n",
    "    task_max_io = 0\n",
    "    prev_file_max_io = 0\n",
    "    file_op_range_dict = {}\n",
    "    for li in stat:\n",
    "        file_key = list(li.keys())[0]\n",
    "        if 'file' in file_key:\n",
    "            # file_stat = list(li.values())[0]\n",
    "            file_stat = li[file_key]\n",
    "            file_name = file_stat['file_name']\n",
    "            file_max_io = 0\n",
    "            file_min_io = prev_file_max_io\n",
    "            file_max_blob = 0\n",
    "            prefetch_type = 0 # 0: no prefetch, 1: prefetch blobs\n",
    "            try:\n",
    "                h5mDraw = file_stat['data']['H5FD_MEM_DRAW']\n",
    "                \n",
    "                if h5mDraw['read_cnt'] > 0:\n",
    "                    # print(f\"blobs here: {[val[1] for val in h5mDraw['read_ranges'].values()]}\")\n",
    "                    max_blob = max([val[1] for val in h5mDraw['read_ranges'].values()])\n",
    "                    max_io = max(h5mDraw['read_ranges'].keys())\n",
    "                    min_io = min(h5mDraw['read_ranges'].keys())\n",
    "                    if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "                    if max_io > task_max_io: task_max_io = max_io\n",
    "                    if max_io > file_max_io: file_max_io = max_io\n",
    "                    if min_io < file_min_io: file_min_io = min_io\n",
    "                    prefetch_type = 1\n",
    "                    \n",
    "                elif h5mDraw['write_cnt'] > 0:\n",
    "                    # print(f\"blobs here: {[val[1] for val in h5mDraw['write_ranges'].values()]}\")\n",
    "                    max_blob = max([val[1] for val in h5mDraw['write_ranges'].values()])\n",
    "                    max_io = max(h5mDraw['write_ranges'].keys())\n",
    "                    min_io = min(h5mDraw['read_ranges'].keys())\n",
    "                    if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "                    if max_io > task_max_io: task_max_io = max_io\n",
    "                    if max_io > file_max_io: file_max_io = max_io\n",
    "                    if min_io < file_min_io: file_min_io = min_io\n",
    "                    prefetch_type = 0\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                for meta_type, meta_stats in file_stat['metadata'].items():\n",
    "                    # print(f\"meta_type: {meta_type}\")\n",
    "                    if file_stat['metadata'][meta_type]['read_cnt'] > 0:\n",
    "                        max_blob = max([val[1] for val in file_stat['metadata'][meta_type]['read_ranges'].values()])\n",
    "                        max_io = max(file_stat['metadata'][meta_type]['read_ranges'].keys())\n",
    "                        min_io = min(file_stat['metadata'][meta_type]['read_ranges'].keys())\n",
    "                        if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "                        if max_io > task_max_io: task_max_io = max_io\n",
    "                        if max_io > file_max_io: file_max_io = max_io\n",
    "                        if min_io < file_min_io: file_min_io = min_io\n",
    "                        prefetch_type = 1\n",
    "                        \n",
    "                    elif file_stat['metadata'][meta_type]['write_cnt'] > 0:\n",
    "                        max_blob = max([val[1] for val in file_stat['metadata'][meta_type]['write_ranges'].values()])\n",
    "                        max_io = max(file_stat['metadata'][meta_type]['write_ranges'].keys())\n",
    "                        min_io = min(file_stat['metadata'][meta_type]['write_ranges'].keys())\n",
    "                        if max_blob > file_max_blob: file_max_blob = max_blob\n",
    "                        if max_io > task_max_io: task_max_io = max_io\n",
    "                        if max_io > file_max_io: file_max_io = max_io\n",
    "                        if min_io < file_min_io: file_min_io = min_io\n",
    "                        prefetch_type = 0\n",
    "                    else:\n",
    "                        pass\n",
    "                \n",
    "                # if \"cloudid_20150506_040000\" in file_name:\n",
    "                #     print(f\"VFD Stat File: {k}\")\n",
    "                #     print(f\"file_name: {file_name}\")\n",
    "                #     print(f\"file_io_range: [{file_min_io},{file_max_io}]\")\n",
    "                if file_name in file_op_range_dict.keys():\n",
    "                    file_op_range_dict[file_name]['io_ranges'] .append((file_min_io, file_max_io))\n",
    "                    if file_max_blob != file_op_range_dict[file_name]['blob_reanges'][1]:\n",
    "                        file_op_range_dict[file_name]['blob_reanges'] = (0, file_max_blob)\n",
    "                        file_op_range_dict[file_name]['prefetch_type'] = prefetch_type\n",
    "                        \n",
    "                else:\n",
    "                    file_op_range_dict[file_name] = {}\n",
    "                    file_op_range_dict[file_name]['io_ranges'] = [(file_min_io, file_max_io)]\n",
    "                    file_op_range_dict[file_name]['blob_reanges'] = (0, file_max_blob)\n",
    "                    file_op_range_dict[file_name]['prefetch_type'] = prefetch_type\n",
    "            \n",
    "                prev_file_max_io = file_max_io\n",
    "                if prefetch_type == 0:\n",
    "                    print(f\"file_op_range_dict: {file_op_range_dict[file_name]}\")\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    file_name = k.split('/')[-1]\n",
    "    task_id = file_name.split('_')[0]\n",
    "    # print(f\"{task_id} Max IO IDX: {task_max_io}\")\n",
    "    range_shift = op_range_shift_dict[task_id]\n",
    "    page_size_factor = 1 # comparing to 1M, 512KB is 2 for 1M\n",
    "    \n",
    "    for file_name, ranges_val in file_op_range_dict.items():\n",
    "        for range in ranges_val['io_ranges']:\n",
    "            pre_entry = {}\n",
    "            # pre_entry['op_count_range'] = f\"[{page_size_factor*(range_shift+0)}, {page_size_factor*(range_shift+task_max_io)}]\"\n",
    "            # pre_entry['op_count_range'] = [page_size_factor*(range_shift+range[0]), page_size_factor*(range_shift+range[1])]\n",
    "            pre_entry['op_count_range'] = [page_size_factor*(range_shift), page_size_factor*(range_shift)]\n",
    "\n",
    "\n",
    "            pre_entry['prefetch'] = []\n",
    "            bucket_entry = {}\n",
    "            bucket_entry['bucket'] = file_name\n",
    "            # bucket_entry['promote_blobs'] = f\"[{page_size_factor*(ranges_val['blob_reanges'][0])}, {page_size_factor*(ranges_val['blob_reanges'][1])}]\"\n",
    "            bucket_entry['blob_ranges'] = [page_size_factor*(page_size_factor*ranges_val['blob_reanges'][0]), page_size_factor*(page_size_factor*ranges_val['blob_reanges'][1])]\n",
    "            bucket_entry['prefetch_type'] = \"promote_blobs\" if ranges_val['prefetch_type'] == 1 else \"demote_blobs\"\n",
    "            \n",
    "            pre_entry['prefetch'].append(bucket_entry)\n",
    "            prefetch_entry_list.append(pre_entry)\n",
    "            \n",
    "\n",
    "with open('./apriori_p-1p9f-512KB.yml', 'w') as outfile:\n",
    "    outfile.writelines(\"0:\\n\")\n",
    "    sorted_entries = sorted(prefetch_entry_list, key=lambda x: x['op_count_range'][0])\n",
    "\n",
    "    for ent in sorted_entries:\n",
    "        # print(f\"  - op_count_range: {ent['op_count_range']}\")\n",
    "        # print(f\"    prefetch:\")\n",
    "        \n",
    "        outfile.writelines(f\"  - op_count_range: {ent['op_count_range']}\\n\")\n",
    "        outfile.writelines(f\"    prefetch:\\n\")\n",
    "        for bucket in ent['prefetch']:\n",
    "            # print(f\"      - bucket: \\\"{bucket['bucket']}\\\"\")\n",
    "            # print(f\"        promote_blobs: {bucket['promote_blobs']}\")\n",
    "            # TODO: bucketname correction\n",
    "            prefix = bucket['bucket'].split('_')[0]\n",
    "            if prefix == \"cloudid\" or prefix == \"track\":\n",
    "                prefix_path = \"/qfs/projects/oddite/tang584/flextrkr_runs/hm_wrf_tbradar/tracking\"\n",
    "            if prefix == \"mcs\" or prefix == \"trackstats\":\n",
    "                prefix_path = \"/qfs/projects/oddite/tang584/flextrkr_runs/hm_wrf_tbradar/stats\"\n",
    "            if prefix == \"wrfout\" or prefix == \"wrf\":\n",
    "                prefix_path = \"/qfs/projects/oddite/tang584/flextrkr_runs/hm_input_data/wrf_tbradar\"\n",
    "            bucket_name = prefix_path + \"/\" + bucket['bucket']\n",
    "            outfile.writelines(f\"      - bucket: \\\"{bucket_name}\\\"\\n\")\n",
    "            outfile.writelines(f\"        {bucket['prefetch_type']}: {bucket['blob_ranges']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
